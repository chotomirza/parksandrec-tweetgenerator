{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parks and Recreation Tweet Generator\n",
    "\n",
    "### Evan Hiroshige, Kira Traynor, Mirza Ahmed\n",
    "\n",
    "#### Running Notebook\n",
    "1. Ensure that\"sorted_name_all.csv\" is located in the same directory as this notebook\n",
    "2. Run notebook from top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant values for character names\n",
    "leslie = \"Leslie Knope\"\n",
    "tom = \"Tom Haverford\"\n",
    "april = \"April Ludgate\"\n",
    "ron = \"Ron Swanson\"\n",
    "perd = \"Perd Hapley\"\n",
    "chris = \"Chris Traeger\"\n",
    "jean = \"Jean-Ralphio Saperstein\"\n",
    "characters = [leslie, tom, april, ron, perd, chris, jean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_character(character, s_token_count):\n",
    "    \"\"\"\n",
    "        Parses through csv file of all Parks and Rec dialogue, only keeping given characters dialogue\n",
    "        Each sentence is padded with s_token_count of start and end sentence tokens\n",
    "        e.g. s_token_count = 2 => [<s> <s> w1 ....  </s> </s>]\n",
    "        The sentences are shuffled so the data can be broken up into training and testing sets where data is \n",
    "        evenly spread over the entire series\n",
    "        Returns shuffled formatted sentences \n",
    "    \"\"\"\n",
    "    all_chars = open(\"sorted_name_all.csv\", newline='')\n",
    "    reader = csv.reader(all_chars, delimiter=\",\", quotechar='\"')\n",
    "    next(reader, None)\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        if row[0] == character:\n",
    "            data.append(row)\n",
    "    all_chars.close()\n",
    "\n",
    "    sentences = []\n",
    "    total_count = 0\n",
    "    processed_count = 0\n",
    "    start = [\"<s>\" for i in range(s_token_count)]\n",
    "    end = [\"</s>\" for i in range(s_token_count)]\n",
    "    for line in data:\n",
    "        text = line[1].lower()\n",
    "        text = text.split()\n",
    "        clean_text = []\n",
    "        total_count += len(text)\n",
    "        for word in text:\n",
    "            no_grammar = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "            if len(no_grammar) == 0:\n",
    "                continue\n",
    "            clean_text.append(no_grammar)\n",
    "        sentence = start + clean_text + end\n",
    "        sentences.append(sentence)\n",
    "    random.seed(0)\n",
    "    random.shuffle(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training/testing files for statistical model\n",
    "\n",
    "NGRAM = 4\n",
    "\n",
    "def create_data_txt(data, character):\n",
    "    \"\"\"\n",
    "        Creates training and testing data for a statistical model\n",
    "    \"\"\"\n",
    "    train = data[:2*len(data)//3]\n",
    "    test = data[2*len(data)//3:]\n",
    "    f = open(f'fourgram-{character}-train.txt', \"w\")\n",
    "    for sentence in train:\n",
    "        line = \" \".join(sentence)\n",
    "        f.write(line + '\\n')\n",
    "    f.close\n",
    "    f = open(f'fourgram-{character}-test.txt', \"w\")\n",
    "    for sentence in test:\n",
    "        line = \" \".join(sentence)\n",
    "        f.write(line + '\\n')\n",
    "    f.close\n",
    "    \n",
    "    \n",
    "def create_statistical_files():\n",
    "    for character in characters:\n",
    "        data = load_character(character, NGRAM - 1)\n",
    "        create_data_txt(data, character)\n",
    "        \n",
    "create_statistical_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.metrics import TopKCategoricalAccuracy\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import numpy as np \n",
    "\n",
    "def get_tokenizer_and_encoded(data):\n",
    "    \"\"\" \n",
    "        Creates a tokenizer and fits it on the given data\n",
    "        Encodes data with tokenizer\n",
    "        Returns tokenizer and encoded data\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    encoded = tokenizer.texts_to_sequences(data)\n",
    "    return (tokenizer, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_data(tokenizer, encoded):\n",
    "    \"\"\"\n",
    "        Creates training and testing data from encoded sentences\n",
    "        Breaks up sentences into sequences from w1:wN for N=1 => N=len(sentence)\n",
    "        All sequences padded with zeroes to length = max length of sentence\n",
    "        Training data is 75% of data, testing data 25%\n",
    "        Returns (trainX, trainY, testX, testY)\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    for i in range(len(encoded)):\n",
    "        encoded_sent = encoded[i]\n",
    "        for k in range(1, len(encoded_sent)):\n",
    "            x.append(encoded_sent[:k])\n",
    "            y.append(encoded_sent[k])\n",
    "\n",
    "    maxlen = max([len(sent) for sent in x])\n",
    "    x = np.array([pad_sequences([sent], maxlen=maxlen, padding='pre')[0] for sent in x])\n",
    "    y = np.array(y)\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    trainX = x[:3*len(x)//4]\n",
    "    trainY = y[:3*len(x)//4]\n",
    "    testX = x[3*len(y)//4:]\n",
    "    testY = y[3*len(y)//4:]\n",
    "    return (trainX, trainY, testX, testY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, maxlen):\n",
    "    \"\"\"\n",
    "        Creates RNN model with Embedding, LSTM, and softmax layers\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 200, input_length=maxlen))\n",
    "    model.add(LSTM(400))\n",
    "    model.add(Dense(vocab_size, activation='softmax')) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainX, trainY, model):\n",
    "    \"\"\"\n",
    "        Trains given model on given data\n",
    "    \"\"\"\n",
    "    acc = TopKCategoricalAccuracy(k=5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[acc, \"accuracy\"])\n",
    "    model.fit(trainX, trainY, epochs=25, batch_size=256)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_word(tokenizer):\n",
    "    \"\"\"\n",
    "        Creates dictionary mapping word index to word using given tokenizer\n",
    "    \"\"\"\n",
    "    index_to_word_dict = {}\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        index_to_word_dict[index] = word\n",
    "    return index_to_word_dict\n",
    "\n",
    "def generate_text(seed, sentence_length, model, maxlen, n, tokenizer, index_to_word):\n",
    "    \"\"\" \n",
    "        Generates n sentences with max length sentence_length. Uses given seed to begin generating words. \n",
    "        Returns sentences\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    while len(output) < n:\n",
    "        seed_text = seed\n",
    "        for _ in range(sentence_length):\n",
    "            tokenized_seed = [tokenizer.word_index[word] for word in seed_text.split()]\n",
    "            tokenized_seed = pad_sequences([tokenized_seed], maxlen=maxlen, padding='pre')\n",
    "            prediction  = model.predict([tokenized_seed])[0]\n",
    "            index = np.random.choice(len(prediction), p=prediction)\n",
    "            \n",
    "            if index == 0:\n",
    "                continue\n",
    "            \n",
    "            predicted_word = index_to_word[index]\n",
    "            seed_text += \" \" + predicted_word\n",
    "            if predicted_word == \"</s>\":\n",
    "                break\n",
    "            if len(seed_text) > 147:\n",
    "                break\n",
    "        if len(seed_text) < 15:\n",
    "            continue\n",
    "        output.append(seed_text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(character, sentences):\n",
    "    \"\"\"\n",
    "        Saves generated sentences to file for specific character\n",
    "    \"\"\"\n",
    "    f = open(f'{character}-rnn-sentences.txt', \"w\")\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def create_and_train_model(character):\n",
    "    \"\"\"\n",
    "        Loads data, trains model, and generates 50 sentences for specific character\n",
    "    \"\"\"\n",
    "    print(\"CHARACTER:\", character)\n",
    "    \n",
    "    # Create necessary variabled\n",
    "    data = load_character(character, 1)\n",
    "    tokenizer, encoded = get_tokenizer_and_encoded(data)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    trainX, trainY, testX, testY = get_training_data(tokenizer, encoded)\n",
    "    maxlen = len(trainX[0])\n",
    "    \n",
    "    # Train model\n",
    "    model = create_model(vocab_size, maxlen)\n",
    "    train_model(trainX, trainY, model)\n",
    "    \n",
    "    # Test model\n",
    "    results = model.evaluate(testX, testY, batch_size=256)\n",
    "    print(f'Results - Loss: {results[0]}, Top-K Accuracy: {results[1]}, Accuracy:{results[2]} ')\n",
    "    \n",
    "    # Generate sentences\n",
    "    index_to_word_dict = index_to_word(tokenizer)\n",
    "    text = generate_text(\"<s>\", 40, model, maxlen, 50, tokenizer, index_to_word_dict)\n",
    "    save(character, text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARACTER: Leslie Knope\n",
      "Epoch 1/25\n",
      "417/417 [==============================] - 310s 741ms/step - loss: 6.4744 - top_k_categorical_accuracy: 0.2315 - accuracy: 0.1292\n",
      "Epoch 2/25\n",
      "360/417 [========================>.....] - ETA: 42s - loss: 5.4478 - top_k_categorical_accuracy: 0.3033 - accuracy: 0.1640"
     ]
    }
   ],
   "source": [
    "# Intentionally breaking up model creation and training for each character \n",
    "# Easier to read and interpret the results\n",
    "create_and_train_model(leslie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(tom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(april)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(ron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(perd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_model(jean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistical_model import LanguageModel, test_model\n",
    "\n",
    "def create_and_train_stat_model(character):\n",
    "    \"\"\"\n",
    "        Builds and trains statistical 4-gram language model for given character\n",
    "        Tests model, calcualtes probaility and std dev, generates 50 sentences\n",
    "    \"\"\"\n",
    "    lm = LanguageModel(NGRAM, True)\n",
    "    lm.train(f'fourgram-{character}-train.txt')\n",
    "    test_model(lm, f'fourgram-{character}-test.txt', character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, separated each model into own cell to improve readability\n",
    "create_and_train_stat_model(leslie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(tom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(april)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(perd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(ron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(chris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_train_stat_model(jean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
